import {
  type StreamStats,
  type VideoData,
  getInitialStreamStats,
  getInitialVideoData,
} from "@/dashboard/store/media-channel-store/video-store"
import {VideoStoreManager} from "@/dashboard/store/media-channel-store/video-store-manager.ts"
import {Box, Flex, IconButton} from "@chakra-ui/react"
import * as deepLab from "@tensorflow-models/deeplab"
import type {DeepLabOutput} from "@tensorflow-models/deeplab/dist/types"
import * as tf from "@tensorflow/tfjs"
import type React from "react"
import {useCallback} from "react"
import {useEffect, useRef, useState} from "react"
import {VideoSegmentationSetting} from "./VideoSegmentationSetting"
import {WidgetFrame} from "../WidgetFrame"

const deeplabModelBase = "pascal"

export interface VideoSegmentationWidgetConfig {
  stream_id: string
  tf_model: string
}

interface VideoSegmentationWidgetProps {
  robotId: string
  config: VideoSegmentationWidgetConfig
  onUpdateConfig?: (newConfig: VideoSegmentationWidgetConfig) => void
  setOpenSetting?: (openSetting: boolean) => void
  onRemove?: () => void
}

interface ErrorWidgetProps extends VideoSegmentationWidgetProps {
  openSetting: boolean
  message: string
}

const ErrorWidget: React.FC<ErrorWidgetProps> = ({
                                                   robotId,
                                                   config,
                                                   onUpdateConfig,
                                                   openSetting,
                                                   setOpenSetting,
                                                   onRemove,
                                                   message,
                                                 }) => (
  <>
    <WidgetFrame
      title="Video Segmentation"
      robot_id={robotId}
      isConnected={false}
      footerMessage={message}
      onSettingClick={() => {
        setOpenSetting?.(true)
      }}
      onRemove={onRemove}
    >
      <Flex
        direction="column"
        align="center"
        justify="center"
        h="100%"
        color="red.500"
        textAlign="center"
      >
        <Box fontSize="2xl" mb={2}>
          âš ï¸
        </Box>
        <Box fontSize="sm">{message}</Box>
      </Flex>
    </WidgetFrame>
    {openSetting && (
      <VideoSegmentationSetting
        isOpen={openSetting}
        config={config}
        onUpdateConfig={onUpdateConfig}
        onClose={() => {
          setOpenSetting?.(false)
        }}
      />
    )}
  </>
)

export const VideoSegmentationWidget: React.FC<
  VideoSegmentationWidgetProps
> = ({robotId, config, onUpdateConfig, onRemove}) => {
  const videoStoreManager = VideoStoreManager.getInstance()

  const [openSetting, setOpenSetting] = useState(false)
  const videoRef = useRef<HTMLVideoElement>(null)
  const canvasRef = useRef<HTMLCanvasElement>(null)
  const [videoData, setVideoData] = useState<VideoData | null>(
    getInitialVideoData(),
  )
  const [streamStats, setStreamStats] = useState<StreamStats>(
    getInitialStreamStats(),
  )
  const [isPlaying, setIsPlaying] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [isConnected, setIsConnected] = useState(false)

  // Segmentation ê´€ë ¨ ìƒíƒœ
  const [model, setModel] = useState<deepLab.SemanticSegmentation | null>(null)
  const [isModelLoading, setIsModelLoading] = useState(true)
  const [isSegmenting, setIsSegmenting] = useState(false)
  const animationFrameRef = useRef<number | null>(null)

  const store = videoStoreManager.createVideoStoreByMediaTypeAuto(
    robotId,
    config?.stream_id,
  )

  // TensorFlow.js ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë“œ
  useEffect(() => {
    const initializeTensorFlow = async () => {
      try {
        setIsModelLoading(true)

        // TensorFlow.js ë°±ì—”ë“œ ì´ˆê¸°í™”
        await tf.ready()

        // DeepLab v3 ëª¨ë¸ ë¡œë“œ
        const loadedModel = await deepLab.load({
          base: deeplabModelBase,
          quantizationBytes: 2,
        })
        setModel(loadedModel)
        setIsModelLoading(false)
        console.log("âœ… DeepLab model loaded successfully")
      } catch (err) {
        console.error("âŒ TensorFlow initialization failed:", err)
        setError("Failed to initialize TensorFlow.js or load model")
        setIsModelLoading(false)
      }
    }

    initializeTensorFlow()
  }, [])

  // ìº”ë²„ìŠ¤ ì„¤ì •
  const setupCanvas = useCallback(() => {
    if (videoRef.current && canvasRef.current) {
      const canvas = canvasRef.current
      const video = videoRef.current

      // videoì˜ ì‹¤ì œ ë¹„ë””ì˜¤ í¬ê¸°
      const videoWidth = video.videoWidth
      const videoHeight = video.videoHeight

      // video elementì˜ display í¬ê¸°
      const elementWidth = video.offsetWidth
      const elementHeight = video.offsetHeight

      // object-fit: containìœ¼ë¡œ ì¸í•œ ì‹¤ì œ í‘œì‹œ ì˜ì—­ ê³„ì‚°
      const videoAspectRatio = videoWidth / videoHeight
      const elementAspectRatio = elementWidth / elementHeight

      let displayWidth: number
      let displayHeight: number
      let offsetX: number
      let offsetY: number

      if (videoAspectRatio > elementAspectRatio) {
        // ë¹„ë””ì˜¤ê°€ ë” ë„“ìŒ - ê°€ë¡œê°€ ê½‰ ì°¸
        displayWidth = elementWidth
        displayHeight = elementWidth / videoAspectRatio
        offsetX = 0
        offsetY = (elementHeight - displayHeight) / 2
      } else {
        // ë¹„ë””ì˜¤ê°€ ë” ë†’ìŒ - ì„¸ë¡œê°€ ê½‰ ì°¸
        displayWidth = elementHeight * videoAspectRatio
        displayHeight = elementHeight
        offsetX = (elementWidth - displayWidth) / 2
        offsetY = 0
      }

      // ìº”ë²„ìŠ¤ í¬ê¸°ë¥¼ ì‹¤ì œ ë¹„ë””ì˜¤ í•´ìƒë„ë¡œ ì„¤ì •
      canvas.width = videoWidth
      canvas.height = videoHeight

      // ìº”ë²„ìŠ¤ ìŠ¤íƒ€ì¼ì„ ì‹¤ì œ í‘œì‹œ ì˜ì—­ì— ë§ê²Œ ì„¤ì •
      canvas.style.width = `${displayWidth}px`
      canvas.style.height = `${displayHeight}px`
      canvas.style.position = "absolute"
      canvas.style.top = `${offsetY + 12}px`
      canvas.style.left = `${offsetX + 12}px`
    }
  }, [])

  const nextFrame = () => {
    animationFrameRef.current = requestAnimationFrame(() => {
      setTimeout(segmentObjects, 100)
    })
  }

  const segmentObjects = useCallback(async () => {
    if (!isSegmenting || !model || !videoRef.current || !canvasRef.current) {
      nextFrame()
      return
    }

    try {
      const video = videoRef.current
      const canvas = canvasRef.current
      const tempCanvas = document.createElement("canvas")
      const canvasCtx = canvas.getContext("2d")
      const tempCanvasCtx = tempCanvas.getContext("2d")

      if (!canvasCtx || !tempCanvasCtx || video.readyState !== 4) {
        nextFrame()
        return
      }

      const segmentations = await model.segment(video)

      canvasCtx.clearRect(0, 0, canvas.width, canvas.height)
      setupCanvas()

      drawSegmentations(
        canvas,
        canvasCtx,
        tempCanvas,
        tempCanvasCtx,
        segmentations,
      )
    } catch (err) {
      console.error("Detection error:", err)
    }

    // ë‹¤ìŒ í”„ë ˆì„ ìŠ¤ì¼€ì¤„ë§ (10FPS)
    if (isSegmenting && isPlaying) {
      nextFrame()
    }
  }, [isSegmenting, isPlaying, model])

  // íƒì§€ ê²°ê³¼ ê·¸ë¦¬ê¸°
  const drawSegmentations = (
    canvas: HTMLCanvasElement,
    canvasCtx: CanvasRenderingContext2D,
    tempCanvas: HTMLCanvasElement,
    tempCanvasCtx: CanvasRenderingContext2D,
    {width, height, segmentationMap}: DeepLabOutput,
  ) => {
    tempCanvas.width = width
    tempCanvas.height = height

    const imageData = tempCanvasCtx.createImageData(width, height)
    const data = imageData.data

    // RGBA í˜•íƒœì˜ segmentationMapì„ 4ì”© ê±´ë„ˆë›°ë©° ì²˜ë¦¬
    for (let i = 0; i < segmentationMap.length; i += 4) {
      const r = segmentationMap[i] // Red
      const g = segmentationMap[i + 1] // Green
      const b = segmentationMap[i + 2] // Blue
      // const a = segmentationMap[i + 3] // Alpha

      const pixelIndex = i // ì´ë¯¸ 4ë°°ìˆ˜ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©

      // ë°°ê²½ì´ ì•„ë‹Œ ê²½ìš° (ê²€ì€ìƒ‰ì´ ì•„ë‹Œ ê²½ìš°)
      if (r !== 0 || g !== 0 || b !== 0) {
        data[pixelIndex] = r
        data[pixelIndex + 1] = g
        data[pixelIndex + 2] = b
        data[pixelIndex + 3] = 130 // ë°˜íˆ¬ëª…
      } else {
        // ë°°ê²½ì€ íˆ¬ëª…í•˜ê²Œ
        data[pixelIndex] = 0
        data[pixelIndex + 1] = 0
        data[pixelIndex + 2] = 0
        data[pixelIndex + 3] = 0
      }
    }

    // ì„ì‹œ ìº”ë²„ìŠ¤ì— ê·¸ë¦¬ê¸°
    tempCanvasCtx.putImageData(imageData, 0, 0)

    // ë¹„ë””ì˜¤ ìº”ë²„ìŠ¤ í¬ê¸°ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§í•´ì„œ ê·¸ë¦¬ê¸°
    canvasCtx.drawImage(
      tempCanvas,
      0,
      0,
      width,
      height,
      0,
      0,
      canvas.width,
      canvas.height,
    )
  }

  // íƒì§€ ìë™ ì‹œì‘
  useEffect(() => {
    if (model && isConnected) {
      setIsSegmenting(true)
      // segmentObjects í˜¸ì¶œì„ ë‹¤ìŒ ë Œë”ë§ ì‚¬ì´í´ë¡œ ì§€ì—°
      const timer = setTimeout(segmentObjects, 0)
      return () => clearTimeout(timer)
    }
    setIsSegmenting(false)
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = null
    }
    // ìº”ë²„ìŠ¤ í´ë¦¬ì–´
    if (canvasRef.current) {
      const ctx = canvasRef.current.getContext("2d")
      ctx?.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height)
    }
  }, [model, isConnected, segmentObjects])

  const configureVideo = () => {
    if (!store) {
      return () => {
      }
    }

    // ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ ì„¤ì •
    if (videoRef.current) {
      store.setVideoElement(videoRef.current)
    }

    // MediaStream ìƒíƒœ í™•ì¸
    const mediaStream = store.getMediaStream()

    if (mediaStream) {
      // MediaStreamì´ ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ì— ì—°ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸
      if (videoRef.current && videoRef.current.srcObject === mediaStream) {
        // MediaStreamì´ ì—°ê²°ë˜ë©´ ìë™ ì¬ìƒ ì‹œë„
        if (videoRef.current.paused) {
          videoRef.current.play().catch((error) => {
            console.error("âŒ ë¹„ë””ì˜¤ ìë™ ì¬ìƒ ì‹¤íŒ¨:", error)
          })
        }
      }
    }

    // ë¹„ë””ì˜¤ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    const videoElement = videoRef.current
    if (videoElement) {
      const handlePlay = () => {
        setIsPlaying(true)
      }
      const handlePause = () => {
        setIsPlaying(false)
      }
      const handleLoadedMetadata = () => {
        setError(null)
        setupCanvas()
      }
      const handleError = (e: any) => {
        console.error("ë¹„ë””ì˜¤ ë¡œë“œ ì˜¤ë¥˜:", e)
        setError("ë¹„ë””ì˜¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
      }

      videoElement.addEventListener("play", handlePlay)
      videoElement.addEventListener("pause", handlePause)
      videoElement.addEventListener("loadedmetadata", handleLoadedMetadata)
      videoElement.addEventListener("error", handleError)

      return () => {
        videoElement.removeEventListener("play", handlePlay)
        videoElement.removeEventListener("pause", handlePause)
        videoElement.removeEventListener("loadedmetadata", handleLoadedMetadata)
        videoElement.removeEventListener("error", handleError)
      }
    }

    return () => {
      // ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ê°€ ë³€ê²½ë˜ë©´ ì´ì „ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì œê±°
      if (videoRef.current) {
        videoRef.current.srcObject = null
      }
    }
  }

  useEffect(() => {
    if (!store) return

    const unsubscribe = store.subscribe((data) => {
      setVideoData(data)
      setIsConnected(data.isActive)
      setError(null)
      if (data.isActive) {
        configureVideo()
      }
    })

    const unscribeStreamStats = store.subscribeStreamStats((stats) => {
      setStreamStats(stats)
    })

    // ì´ˆê¸° ë°ì´í„° ì„¤ì •
    if (store.getMediaStream()) {
      const initialData: VideoData = {
        streamId: store.getMediaStream()?.id || "",
        robotId: store.getRobotId(),
        channelLabel: store.getChannelLabel(),
        mediaStream: store.getMediaStream()!,
        isActive: store.isStreamActive(),
        timestamp: Date.now(),
      }
      setVideoData(initialData)
      setIsConnected(store.isStreamActive())
    }

    const closing = configureVideo()

    return () => {
      closing()
      unsubscribe()
      unscribeStreamStats()
    }
  }, [store, videoRef.current])

  const handlePlayPause = () => {
    if (videoRef.current) {
      if (isPlaying) {
        videoRef.current.pause()
      } else {
        videoRef.current.play()
      }
    }
  }

  if (!config || !config.stream_id) {
    return (
      <ErrorWidget
        robotId={robotId}
        config={config}
        onUpdateConfig={onUpdateConfig}
        openSetting={openSetting}
        setOpenSetting={setOpenSetting}
        onRemove={onRemove}
        message={"No stream ID configured"}
      />
    )
  }

  if (!store) {
    return (
      <ErrorWidget
        robotId={robotId}
        config={config}
        onUpdateConfig={onUpdateConfig}
        openSetting={openSetting}
        setOpenSetting={setOpenSetting}
        onRemove={onRemove}
        message={"No video stream available"}
      />
    )
  }

  // Footer info
  const footerInfo =
    videoData && streamStats
      ? [
        {
          label: "FPS",
          value: `${streamStats.fps} fps`,
        },
        {
          label: "Stream ID",
          value: videoData.streamId,
        },
        {
          label: "Model",
          value: config.tf_model,
        },
      ]
      : []

  const getStatusMessage = () => {
    if (isModelLoading) return "Loading AI model..."
    if (!isConnected) return "Waiting for stream..."
    if (isSegmenting) return "Segmenting objects..."
    return "Video stream active"
  }

  return (
    <>
      <WidgetFrame
        title="Video Segmentation"
        robot_id={robotId}
        isConnected={isConnected}
        footerInfo={footerInfo}
        footerMessage={getStatusMessage()}
        onSettingClick={() => {
          setOpenSetting?.(true)
        }}
        onRemove={onRemove}
      >
        {error ? (
          <Flex
            direction="column"
            align="center"
            justify="center"
            h="100%"
            color="red.500"
            textAlign="center"
          >
            <Box fontSize="2xl" mb={2}>
              âš ï¸
            </Box>
            <Box fontSize="sm">{error}</Box>
          </Flex>
        ) : (
          <>
            <video
              ref={videoRef}
              style={{
                width: "100%",
                height: "100%",
                objectFit: "contain",
                borderRadius: "8px",
              }}
              playsInline
              muted
              autoPlay
            />

            {/* Segmentation Canvas Overlay */}
            <canvas
              ref={canvasRef}
              style={{
                pointerEvents: "none",
                borderRadius: "8px",
              }}
            />

            {/* ë¹„ë””ì˜¤ ì»¨íŠ¸ë¡¤ ì˜¤ë²„ë ˆì´ */}
            <Box
              position="absolute"
              bottom={0}
              left={0}
              right={0}
              bg="linear-gradient(to top, rgba(0,0,0,0.7), transparent)"
              p={3}
              opacity={0}
              _hover={{opacity: 1}}
              transition="opacity 0.2s"
            >
              <Flex justify="center" align="center" gap={2}>
                <IconButton
                  size="sm"
                  colorScheme="whiteAlpha"
                  onClick={handlePlayPause}
                  aria-label={isPlaying ? "Pause" : "Play"}
                >
                  {isPlaying ? "â¸ï¸" : "â–¶ï¸"}
                </IconButton>
              </Flex>
            </Box>

            {/* ëª¨ë¸ ë¡œë”© ì¸ë””ì¼€ì´í„° */}
            {isModelLoading && (
              <Flex
                position="absolute"
                top={0}
                left={0}
                right={0}
                bottom={0}
                bg="rgba(0, 0, 0, 0.5)"
                align="center"
                justify="center"
                color="white"
              >
                <Box textAlign="center">
                  <Box fontSize="2xl" mb={2}>
                    ğŸ¤–
                  </Box>
                  <Box fontSize="sm">Loading AI model...</Box>
                </Box>
              </Flex>
            )}
          </>
        )}
      </WidgetFrame>
      {openSetting && (
        <VideoSegmentationSetting
          isOpen={openSetting}
          config={config}
          onUpdateConfig={onUpdateConfig}
          onClose={() => {
            setOpenSetting?.(false)
          }}
        />
      )}
    </>
  )
}
